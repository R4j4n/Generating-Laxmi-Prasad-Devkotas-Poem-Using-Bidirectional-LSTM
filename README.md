# Generating Laxmi Prasad Devkota's Poem Using Bidirectional LSTM


[![Made withJupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?style=for-the-badge&logo=Jupyter)](https://jupyter.org/try)<br>
[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://forthebadge.com)

Result produced by model:

 ## "&nbsp;&nbsp;рдордирдХреЛ рд╡реНрдпрдерд╛ рд░рд╣рдиреНрдЫ рдХрд╣рд╛рдБ рдорд▓рд╛рдИ рдмрддрд╛рдК
## &nbsp; &nbsp;рдЖрдорд╛ рдХрд▓реЗрдЬрд╛ рдЪрд┐рдЫреНрд░реНрдпреМ рдирд┐ рддрд░ рддреА
## &nbsp;&nbsp;&nbsp;рдорд╛рдпрд╛ рдпреЛ рдирд┐рд╢рд╛ рджреНрдпреМ рдкреНрд░рд╛рдг рддрд┐рдореА рдЫ
## &nbsp;&nbsp;&nbsp;рддрд┐рдореА рдЫреМ рдкрд░ рднрдиреНрдиреЗ рдпреЛ рдбрд░ рдорд▓рд╛рдИ
## &nbsp;&nbsp;&nbsp;рдЬреБрди рд╣реЛ рд╡рд╛рд╕ рдХреЗрд╡рд▓ рдкрд╣реЗрдБрд▓рд╛ рдлреВрд▓ &nbsp;&nbsp; "

<br>
<br> 

## Generating Sequence of N-gram Tokens : 

Language modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.

The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. PythonтАЩs library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens.

Lets take a look at one of the input Sequence: 

[[1337, 2623],<br>
 [1337, 2623, 12],<br>
 [1337, 2623, 12, 39],<br>
 [1337, 2623, 12, 39, 103]]

 In the above output [1337, 2623], [1337, 2623, 12], [1337, 2623, 12, 39] and so on represents the ngram phrases generated from the input data, where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example
 <br>

### Line: рдирдЫрд╛рдбреА рдЬрд╛рдиреЛрд╕реН рд╣реЗ рдореЗрд░рд╛ рдкреНрд░рд╛рдг
### Ngrams: | Sequence of Tokens

рдирдЫрд╛рдбреА рдЬрд╛рдиреЛрд╕реН рд╣реЗ рдореЗрд░рд╛ рдкреНрд░рд╛рдг

|  Ngram |Sequence of tokens   |   |   |   |
|---|---|---|---|---|
| рдирдЫрд╛рдбреА рдЬрд╛рдиреЛрд╕реН  | [1337, 2623]  |   
|рдирдЫрд╛рдбреА рдЬрд╛рдиреЛрд╕реН рд╣реЗ   | [1337, 2623, 12]  |   
|рдирдЫрд╛рдбреА рдЬрд╛рдиреЛрд╕реН рд╣реЗ рдореЗрд░рд╛  | [1337, 2623, 12, 39]  |   
|рдирдЫрд╛рдбреА рдЬрд╛рдиреЛрд╕реН рд╣реЗ рдореЗрд░рд╛ рдкреНрд░рд╛рдг |[1337, 2623, 12, 39, 103]|


## Bidirectional LSTM Model:

![alt text](BILSTM_100.png "title")

## Dataset: 
```You can find the original dataset ``` [ЁЯУ░ЁЯУ░HEREЁЯУ░ЁЯУ░](https://github.com/devkotasawal1/Poem-Generator/blob/master/lspd.txt)<br>
```Download pretrained weight from ```[ЁЯУжЁЯУжHEREЁЯУжЁЯУж](https://drive.google.com/file/d/1yvqhGUZWWpk3aVsgP2NKBNCwLfRFzSVA/view?usp=sharing)
